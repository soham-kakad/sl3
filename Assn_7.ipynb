{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6dILJPPm1XA"
   },
   "source": [
    "1. Extract Sample document and apply following document preprocessing methods: tokenization, POS Tagging, stop words removal, stemming and lemmatization\n",
    "2. Create repesentation of documents by calculating Term Frequency and InverseDocumentFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0v-YgzjmgX8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1712981452347,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "Jw1p0dnvBkmy"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from  nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1712981681246,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "L5Rxk5fXBqe3",
    "outputId": "8bb24816-eaf0-4b20-a068-103d4ecb3f70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n",
      "['Hello I am Gayatri Deshmukh.', 'I am from Nanded District.', 'I will be an Engineer in few months.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello I am Gayatri Deshmukh. I am from Nanded District. I will be an Engineer in few months.\"\n",
    "\n",
    "#Tokenization\n",
    "nltk.download('punkt')\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "tokenized_sentences = sent_tokenize(sentence)\n",
    "\n",
    "print(tokenized_words)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1712981683338,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "aFOa2ViiC9iU",
    "outputId": "0d06dfe6-6566-43a0-9964-a8386690c10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version  ['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n",
      "Clean Version ['Hello', 'I', 'Gayatri', 'Deshmukh', '.', 'I', 'Nanded', 'District', '.', 'I', 'Engineer', 'months', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "cleaned_token = []\n",
    "for i in tokenized_words:\n",
    "  if i not in stop_words:\n",
    "    cleaned_token.append(i)\n",
    "\n",
    "print(\"Unclean version \", tokenized_words)\n",
    "print(\"Clean Version\", cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1712981687423,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "_UFln4zoOQf8",
    "outputId": "2b2be4d1-4726-4c28-adfc-a5942014909a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'i', 'am', 'gayatri', 'deshmukh', '.', 'i', 'am', 'from', 'nand', 'district', '.', 'i', 'will', 'be', 'an', 'engin', 'in', 'few', 'month', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "stemmed_words = []\n",
    "for i in tokenized_words:\n",
    "    stemmed = snowball_stemmer.stem(i)\n",
    "    stemmed_words.append(stemmed)\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2445,
     "status": "ok",
     "timestamp": 1712981708744,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "XKbYLwAbq4zl",
    "outputId": "69ed31e6-d2ae-4ce4-ad0c-7d0a8f4bb94a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'month', '.']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = []\n",
    "for i in tokenized_words:\n",
    "    lemmatized = wordnet_lemmatizer.lemmatize(i)\n",
    "    lemmatized_words.append(lemmatized)\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1712981714301,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "6ei5-kNvrFrZ",
    "outputId": "8819f2c1-524d-42a9-e26a-60a192034a2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('Gayatri', 'NNP'), ('Deshmukh', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Nanded', 'NNP'), ('District', 'NNP'), ('.', '.'), ('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('an', 'DT'), ('Engineer', 'NNP'), ('in', 'IN'), ('few', 'JJ'), ('months', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Pos Tagging\n",
    "# dt - determinnant\n",
    "# NN - noun\n",
    "# In - prep / conjunc\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tag = nltk.pos_tag(tokenized_words)\n",
    "\n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1712981854419,
     "user": {
      "displayName": "Riya Jain",
      "userId": "05653870576225209853"
     },
     "user_tz": -330
    },
    "id": "1_ITg5Ud09Hu",
    "outputId": "5a05833d-c649-4825-b65f-fd75b0cb48a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indexing:  {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n",
      "tf-idf in matrix form: \n",
      " [[0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.70710678 0.        ]\n",
      " [0.44036207 0.44036207 0.3349067  0.         0.         0.\n",
      "  0.44036207 0.         0.3349067  0.44036207]\n",
      " [0.         0.         0.37302199 0.49047908 0.37302199 0.49047908\n",
      "  0.         0.49047908 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "d0 = \"Good Morning\"\n",
    "d1 = \"Do daily exercise in the morning \"\n",
    "d2 = \"exercise is good for health\"\n",
    "series = [d0, d1, d2]\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(series)\n",
    "\n",
    "print(\"Word Indexing: \", tfidf.vocabulary_)\n",
    "print(\"tf-idf in matrix form: \\n\", result.toarray())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
